# Motivation

## Why are VLMs important for Robotics?

  Recent improvements on Vision Language Models have showed significant performance on the field of decision making. According to the [Survey of the Foundation Models in Robotics](https://arxiv.org/abs/2312.07843), VLMs (Vision Language Models) were introduced as a valuable tools for enhancing robotic capabilities as well as LLMs. VLMs are often leveraged for RL approaches in the Robotic navigation and manipulation tasks to lead generalized behaviors that is applicable to various tasks.

  [DIAL](https://instructionaugmentation.github.io/), utilized pretrained CLIP (which is VLM) for data augmentation in semi-supervised way. Starting from few annotated dataset it can annotate large unlabeled dataset, which enables significant scalability in generating dataset for robotic manipulation. As similar to DIAL, we also want to use VLMs for dataset augmentation. But, in extend, we seek whether it can be applied in the hierarchical learning setup which requires non-trivial inference of sub-tasks and skill set generation in robotic manipulation tasks.

## What can the base pretrained VLMs do?

  We already know that fundamental models such as GPT-4, Claude 3 can many things with near human performance. There were surge of interest toward [LLaVA](https://llava-vl.github.io/) introduced recently (2023) due to its on-par performance with GPT-4 despite of relatively small parameter size. We conducted preliminary experiment with base LLaVA-1.5v-7b model whether it can understand the underlying nature of the environments we use. Visual observations in the environments were queried to LLaVA with different questions asking for details in each environment.

### LOReL Sawyer robotic manipulator environment

This is one of an expert trajectory on open drawer task. From the entire trajectory of 20 timesteps, (a) is the initial observation and (e) is the final observation. 

![Figure 1. LOReL Sawyer environment. (a) t = 0 (b) t = 8 (c) t= 13 (d) t = 15 (e) t = 19](Motivation%206175634f7f9f4a99b6d822783bf94cd9/Untitled.png)

Figure 1. LOReL Sawyer environment. (a) t = 0 (b) t = 8 (c) t= 13 (d) t = 15 (e) t = 19

We asked following two common questions and additional question according to the context. 

> Q1 : What do you see from the image?
Q2 : Assume you are controlling the robot arm. Your objective is to fully open the green drawer fixed to the table. What will you do next to fully open the drawer?
> 

For those common questions, reasonable but vague and environment irrelevant answers were generated throughout all timesteps as below. 

> A1 : The image shows a 3D rendering of a robotic arm interacting with a green box on a table. 
A2 : To fully open the green drawer, I would first ensure that the robot arm is positioned correctly and has the necessary grip strength to hold the drawer. Then, I would use the arm’s gripper to grasp the handle or edge of the drawer, and carefully open it by applying a gentle pulling motion. The robot would then fully open the drawer, allowing it to be accessed or removed from its current position.
> 

For additional, context related questions, the answers were all incorrect. 

1. Beginning, t = 0, Figure 1 (a).
    
    > Q3 : Is robot gripper holding the handle of the green drawer?
    A3 : Yes, the robot gripper in the image appears to be holding the handle of the green drawer. (incorrect)
    > 
2. Reaching, t = 8, Figure 1 (b).
    
    > Q3 : Is robot gripper holding the handle of the green drawer? 
    A3 : Yes, the robot gripper in the image appears to be holding the handle of the green drawer. (incorrect)
    > 
3. Grabbing, t = 13, Figure 1 (c).
    
    > Q3 : Is robot gripper holding the handle of the drawer? 
    A3 : Yes, the robot gripper is holding the handle of the drawer. This would allow the robot to open the drawer by manipulating the handle. (incorrect)
    > 
4. Pulling, t = 15, Fig 2 (d).
    
    > Q3 : Is the drawer open?
    A3 : Based on the image and the steps I provided earlier, it is not possible to definitively determine whether the drawer is open or not. · · · (incorrect)
    > 
5. End, t = 19, Fig 2 (e).
    
    > Q3 : Is the drawer open?
    A3 : Based on the image, the green drawer is not open. (incorrect)
    > 

We can observe that even smallest LLaVA 7b base model knows about the underlying dynamics about the robot arm, and instruction ‘open the drawer’. However, answers are too general to be applied for this case and details are not correlated. 

### BabyAI environment

This is one of an expert trajectory on BabyAI GoToSeq task, where the agent (red triangle) has to visit certain object in order. The environment is designed to have partial observation of 7x7 grid (gray grids). But to include more environment dynamics into the observation, we used entire observation. 

Instruction for this trajectory was to “go to blue box and go to purple door and go to red box”.

![Figure 2. BabyAI GoToSeq trajectory. (a) Beginning (b) Before blue box (c) Before purple door (d) Before red box](Motivation%206175634f7f9f4a99b6d822783bf94cd9/Untitled%201.png)

Figure 2. BabyAI GoToSeq trajectory. (a) Beginning (b) Before blue box (c) Before purple door (d) Before red box

We asked following two common questions and additional question according to the context. 

> Q1 : What do you see from the image?
> 

Answers of question 1 didn’t changed much throughout the trajectory. 

> A1 : The image appears to be a screenshot from a video game or a puzzle game. It shows a grid-based environment with various colored squares, some of which are highlighted in different colors. There are also some symbols and keys that seem to be part of the game’s interface.
> 

For context specific question, 

1. Beginning, Figure 2. (a)
    
    > Q2 : Can you find the blue box, purple door and red box?
    A2 : The blue box is located in the bottom left corner, the purple door is in the bottom right corner, and the red box is in the top left corner. These are likely part
    of the game’s puzzle or level design. (incorrect)
    > 
2. Before blue box, Figure 2. (b)
    
    > Q2. The red triangle is currently headed north. Where should the red triangle go to reach **blue box**?
    A2. To reach the blue box from the red triangle’s current position, you should move the red triangle to the right. The blue box is located to the right of the red triangle’s current position. (incorrect)
    > 
3. Before purple door, Figure 2. (c)
    
    > Q2. The red triangle is currently headed east. Where should the red triangle go to reach **closed purple door**?
    A2. To reach the closed purple door, the red triangle should head northeast. The current direction of the triangle is east, so it needs to turn to the northeast to align with the door’s position.
    > 
4. Before red box, Figure 3. (d)
    
    > Q2. The red triangle is currently headed north. Where should the red triangle go to reach **red box**?
    A2. To reach the red box, the red triangle should head northeast. The red box is located at the intersection of the grid where the red triangle is currently positioned, and the northeast direction will take the red triangle to the red box. (ambiguous)
    > 

For questions regarding understanding of the context, base model didn’t even understand the agent and the orientation, property of the objects. 

**From these preliminary experiment on LLaVA base model, we concluded that this inference result from base model was promising. If we can provide the model to learn underlying dynamics of the environment, it could potentially applied to generate subgoals.**

[🏠 Home](../Subgoal%20generation%20with%20Vision%20Language%20Models%20for%2071dadafcdb46425d9f40acbb7b606c42.md)

[⇒ Preliminary](Preliminary%20b09adf5576464baf816174dd98b7fccf.md)