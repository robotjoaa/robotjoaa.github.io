# Results

## Baseline Reproduction

  We reproduced results of the SkillDiffuser and LISA to validate codes in the repository are reliable.

The images below are trained behaviors. To see the detailed training result, please refer **[here](Experiment%20Details%202269cadb9f9242f89587e6f0db09b775.md)**. 

1. BabyAI

![GoToSeq : go to a red box and go to a grey door](Results%20eb542b8b5ec840bb872b245719228ef3/0_5.gif)

GoToSeq : go to a red box and go to a grey door

![SynthSeq : pick up a red ball after you pick up a key](Results%20eb542b8b5ec840bb872b245719228ef3/bosslevel1.gif)

SynthSeq : pick up a red ball after you pick up a key

![BossLevel : go to the red ball and open a door](Results%20eb542b8b5ec840bb872b245719228ef3/synthseq1.gif)

BossLevel : go to the red ball and open a door

  We applied LISA in 3 tasks of GoToSeq, SynthSeq and BossLevel. We noticed that there could be chances of relatively easy tasks to be generated. It would a good future direction to divide level generations according to difficulties and and then evaluate. 

1. LOReL 
    
    [[close drawer and turn faucet left and move black mug right](https://gifyu.com/image/SfBq8)](https://gifyu.com/image/SfBq8)
    
    [close drawer and turn faucet left and move black mug right](https://gifyu.com/image/SfBq8)
    
    [[pull the handle and move black mug down](https://gifyu.com/image/SfBq6)](https://gifyu.com/image/SfBq6)
    
    [pull the handle and move black mug down](https://gifyu.com/image/SfBq6)
    

  We observed that in some cases, SkillDiffuser was able to solve complex composed tasks.  

## VLM finetuning

### How does it trained?

  We first applied QLoRA but there were no difference in training time, resulted adaptor model size. There were some unsolved issues with applying QLoRA, so we trained with LoRA until the training loss increased and used best checkpoint around 600 step. Evaluation loss increases between 5~6 epochs, we considered that this is overfitting, no further running.  

![Train loss from train dataset split (11k) and eval loss from validation dataset split (1k)](Results%20eb542b8b5ec840bb872b245719228ef3/Untitled.png)

Train loss from train dataset split (11k) and eval loss from validation dataset split (1k)

It took about 2~3 hours, with 7~8 seconds per step to train. With batch size 32, it requires 75 steps per epoch to iterate all 12k datasets. We ran finetuning on five RTX 4000 SFF Ada each with 20G VRAM. 

### How is it different from the original LLaVA output

  We compared answer from original LLaVA and fine-tuned LLaVA with questions in same format with the input dataset, **but didn‚Äôt used for training(test split of dataset)**. For training dataset, we observed that it matches with our answer in general.

We asked question asking for subgoal and questions asking about the environment. 

![Image queried to the LLaVA with the question. Preferred subgoal will be **‚ÄòGo to the purple door on your left.‚Äô**](Results%20eb542b8b5ec840bb872b245719228ef3/42378712-67c9-4099-81c5-eb61a3c49580.jpg)

Image queried to the LLaVA with the question. Preferred subgoal will be **‚ÄòGo to the purple door on your left.‚Äô**

> Q. In this 2D maze, you have to control red arrow to finish following tasks, go to a key and go to a box, then go to the green key and go to a purple ball. I already finished 'go to a key and go to a box'. Where should the red arrow go next? 
(Expected answer : Go to the purple door on your left)
> 

**[Original]** : Based on the image, the red arrow should go to the green key next. From the current position of the red arrow, it is already close to the green key, so it is the next logical step to move towards it. After reaching the green key, the red arrow can then proceed to the purple ball.

**[Finetuned]** : Go to the grey door on your right. 

> Q. What is the color of the closest ball? (Expected answer : Purple)
> 

**[Original]** : The color of the closest ball is purple. (correct)

**[Finetuned]** : Red. (wrong)

> Q. How many balls are on your right? (Expected answer : no ball, 0)
> 

**[Original]** : There are two balls on your right. (wrong)

**[Finetuned]** : 1 (wrong)

> Q. Assume you are the red arrow headed right. How many balls are on your left? 
(Expected answer : 2)
> 

**[Original]** : As the red arrow, there are two balls on your left (correct)

**[Finetuned]** : 1 (wrong) 

  We can observe that finetuned LLaVA can come up with answers that is similar to the input dataset but does not understand the BabyAI environment, output format **overfitted** to the training dataset. Finetuned model still does not understand what the object, property of the object, and the direction in the scene. Dataset used for training has limited answer formats of ‚ÄòExplore unseen grids ~‚Äô and  ‚ÄòGo to the door ~‚Äô. 

  Overfitted model occasionally outputs blind prediction in similar format for any question, which is frequently viewed. Original LLaVA inferred better for distinguishing color and number of the objects, distance of the object. 

  The training, validation loss during the finetuning does not provide much information to the model‚Äôs performance. Thus, we manually checked by randomly selecting few images whether it matches expected answer. **However, we found out that the result was near random, even with the training dataset. This suggests our dataset are small to derive general inference of the task.**

  From this result, we learned that dataset for fine-tuning should be large and diverse. Questions regarding environment dynamics should have been included, not just subgoal. In the discussion section, further inspections on output label distribution were conducted and concluded that there could have been bias in the dataset.

### Video demonstration of comparing fine-tuned LLaVA (Outputs as above)

  LLaVA official repository offers APIs to iteratively infer trained models. As we have compared before, llava-1.5v-7b which is the base model and finetuned llava-1.5v-7b-lora is available. From the video, we can see that inference speed can be quite fast(under a second).

![Untitled](Results%20eb542b8b5ec840bb872b245719228ef3/Untitled%201.png)

[](https://drive.google.com/file/d/1cuNAy99hKqfvm6VvwVpauPf4EqBDYsGC/view?usp=sharing)

[üè† Home](../Subgoal%20generation%20with%20Vision%20Language%20Models%20for%2071dadafcdb46425d9f40acbb7b606c42.md)

[‚áí Experiment Details](Experiment%20Details%202269cadb9f9242f89587e6f0db09b775.md)